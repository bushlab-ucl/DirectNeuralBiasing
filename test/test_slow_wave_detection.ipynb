{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c0da8-897a-41e0-bbfd-c4f642a86e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brpylib import NsxFile\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import direct_neural_biasing as dnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b270a-1aae-435f-9b42-0e8b64898b46",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73353c9f-767f-4d41-b81d-5c8688a21a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = np.load('data/Patient2EEG.npy')\n",
    "mrk_file = 'data/Patient02_OfflineMrk.mrk'\n",
    "data = data_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc045c93-03f6-49e3-91cf-60f92ee67df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8d803-a765-4521-aaab-22f7e6795e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_stream_section(data_stream, n, m):\n",
    "    \"\"\"\n",
    "    Plots an n-m section of a 1D integer data stream as a line graph.\n",
    "\n",
    "    Args:\n",
    "        data_stream (list or numpy.ndarray): The 1D integer data stream.\n",
    "        n (int): The starting index (inclusive).\n",
    "        m (int): The ending index (inclusive).\n",
    "    \"\"\"\n",
    "    section = data_stream[n : m + 1]\n",
    "    indices = range(n, m + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(indices, section)\n",
    "    plt.title(f'Data Section {n} to {m}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "start_t = 2515\n",
    "end_t = 4515\n",
    "\n",
    "plot_data_stream_section(data, start_t, end_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2da24-28e0-4b65-838a-d09b7756b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Concise .mrk file parser ---\n",
    "def parse_mrk_file_concise(filepath):\n",
    "    \"\"\"\n",
    "    Parses a .mrk file into a dictionary (signal_type: [indices]).\n",
    "    Assumes first line is header, subsequent lines are 'index index signal_type'.\n",
    "    \"\"\"\n",
    "    mrk_data = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        next(f) # Skip header line\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                index = int(parts[0])\n",
    "                signal_type = parts[2]\n",
    "                mrk_data.setdefault(signal_type, []).append(index)\n",
    "    return mrk_data\n",
    "\n",
    "def plot_marker_with_context(data_stream, marker_index, signal_type, context_window=500, output_dir=\"marker_plots\"):\n",
    "    \"\"\"\n",
    "    Plots a single marker with surrounding data context.\n",
    "    \"\"\"\n",
    "    marker_index = int((marker_index / 512) * 30000) # adjust for differe3nces in sample rate\n",
    "    data_length = len(data_stream)\n",
    "    plot_start = max(0, marker_index - context_window)\n",
    "    plot_end = min(data_length - 1, marker_index + context_window)\n",
    "\n",
    "    section_data = data_stream[plot_start : plot_end + 1]\n",
    "    section_indices = range(plot_start, plot_end + 1)\n",
    "\n",
    "    # CORRECTED LINE: Check if the section_data is empty using its length/size\n",
    "    if len(section_data) == 0: # or if section_data.size == 0: if you're sure it's a numpy array\n",
    "        print(f\"Skipping plot for marker {marker_index} ({signal_type}) due to empty data section.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(section_indices, section_data, label='Continuous Data', color='blue', linewidth=1.5)\n",
    "\n",
    "    # Highlight the marker point\n",
    "    plt.axvline(x=marker_index, color='red', linestyle='--', label=f'Marker: {signal_type}')\n",
    "    # Ensure the marker_index is within the bounds of data_stream before trying to access it\n",
    "    if 0 <= marker_index < data_length:\n",
    "        plt.plot(marker_index, data_stream[marker_index], 'ro', markersize=8, label='Marker Location')\n",
    "    else:\n",
    "        print(f\"Warning: Marker index {marker_index} is out of bounds for data_stream. Cannot plot marker point.\")\n",
    "\n",
    "\n",
    "    plt.title(f'Signal Type: {signal_type} at Index: {marker_index} (Context: $\\pm${context_window})')\n",
    "    plt.xlabel('Data Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d902839-094f-473e-9d8d-f2a357596fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Parse the .mrk file\n",
    "parsed_markers = parse_mrk_file_concise(mrk_file)\n",
    "print(\"\\nParsed Markers:\")\n",
    "for signal_type, indices in parsed_markers.items():\n",
    "    print(f\"  {signal_type}: {indices[:5]}... ({len(indices)} total)\")\n",
    "\n",
    "# 2. Plot each marker with context (first 10, show only)\n",
    "context_window_size = 100000 # Adjust this to change how much data is shown around each marker\n",
    "max_plots_to_show = 5\n",
    "plots_shown_count = 0\n",
    "\n",
    "print(f\"\\nGenerating and displaying the first {max_plots_to_show} plots with context window of $\\pm${context_window_size}...\")\n",
    "\n",
    "# Iterate through signal types and their indices\n",
    "for signal_type, indices in parsed_markers.items():\n",
    "    for marker_index in indices:\n",
    "        if plots_shown_count < max_plots_to_show:\n",
    "            plot_marker_with_context(data, marker_index, signal_type, context_window=context_window_size)\n",
    "            plots_shown_count += 1\n",
    "        else:\n",
    "            # Once 10 plots are shown, break out of the inner loop\n",
    "            break\n",
    "    if plots_shown_count >= max_plots_to_show:\n",
    "        # If 10 plots are shown, break out of the outer loop too\n",
    "        break\n",
    "\n",
    "if plots_shown_count == 0:\n",
    "    print(\"No plots were generated. Check your .mrk file or data stream.\")\n",
    "else:\n",
    "    print(f\"\\nFinished displaying the first {plots_shown_count} marker plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d09f6-cd54-4fd2-9caf-a17c710d11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "DirectNeuralBiasing Python Script using YAML Configuration\n",
    "\n",
    "This script expects a NumPy array named 'data' to be defined before \n",
    "the 'main' function is called, or for the data loading section within \n",
    "main() to be uncommented and pointed to a valid data file.\n",
    "\"\"\"\n",
    "\n",
    "import direct_neural_biasing as dnb\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "def create_config_file(config_path=\"config.yaml\"):\n",
    "    \"\"\"Creates a sample YAML configuration file.\"\"\"\n",
    "    config = {\n",
    "        'processor': {'verbose': True, 'fs': 30000.0, 'channel': 1, 'enable_debug_logging': True},\n",
    "        'filters': {\n",
    "            'bandpass_filters': [\n",
    "                {'id': 'slow_wave_filter', 'f_low': 0.5, 'f_high': 4.0},\n",
    "                {'id': 'ied_filter', 'f_low': 80.0, 'f_high': 120.0}\n",
    "            ]\n",
    "        },\n",
    "        'detectors': {\n",
    "            'wave_peak_detectors': [\n",
    "                {'id': 'slow_wave_detector', 'filter_id': 'slow_wave_filter', 'z_score_threshold': 1.0, 'sinusoidness_threshold': 0.7, 'check_sinusoidness': False, 'wave_polarity': 'downwave', 'min_wave_length_ms': 500.0, 'max_wave_length_ms': 2000.0},\n",
    "                {'id': 'ied_detector', 'filter_id': 'ied_filter', 'z_score_threshold': 2.5, 'sinusoidness_threshold': 0.0, 'check_sinusoidness': False, 'wave_polarity': 'upwave', 'min_wave_length_ms': None, 'max_wave_length_ms': None}\n",
    "            ]\n",
    "        },\n",
    "        'triggers': {\n",
    "            'pulse_triggers': [\n",
    "                {'id': 'pulse_trigger', 'activation_detector_id': 'slow_wave_detector', 'inhibition_detector_id': 'ied_detector', 'inhibition_cooldown_ms': 1000.0, 'pulse_cooldown_ms': 0.0}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "    print(f\"Created configuration file: {config_path}\")\n",
    "\n",
    "def load_data_from_file(file_path):\n",
    "    \"\"\"Loads data from a .csv, .npy, or .txt file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        data = np.loadtxt(file_path, delimiter=',')\n",
    "    elif file_path.endswith('.npy'):\n",
    "        data = np.load(file_path)\n",
    "    else:\n",
    "        data = np.loadtxt(file_path)\n",
    "        \n",
    "    print(f\"Loaded {len(data)} samples from {file_path}\")\n",
    "    return data\n",
    "\n",
    "def process_data_with_progress(signal_processor, data, chunk_size=4096, context_size=2000):\n",
    "    \"\"\"Processes data in chunks with a progress bar and gathers event context.\"\"\"\n",
    "    total_samples = len(data)\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: Data array is empty. Nothing to process.\")\n",
    "        return []\n",
    "        \n",
    "    total_chunks = (total_samples + chunk_size - 1) // chunk_size\n",
    "    buffer_size = (context_size * 2) + 1\n",
    "    event_buffer = deque(maxlen=buffer_size)\n",
    "    detected_events = []\n",
    "    \n",
    "    signal_processor.reset_index()\n",
    "\n",
    "    print(f\"Processing {total_samples} samples in {total_chunks} chunks...\")\n",
    "    \n",
    "    last_progress_percent = -1\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        chunk = data[i:i + chunk_size].tolist()\n",
    "        \n",
    "        chunk_output, trigger_timestamp = signal_processor.run_chunk(chunk)\n",
    "        \n",
    "        for sample_result in chunk_output:\n",
    "            event_buffer.append(sample_result)\n",
    "            \n",
    "            if len(event_buffer) == buffer_size:\n",
    "                middle_sample = event_buffer[context_size]\n",
    "                if middle_sample.get(\"triggers:pulse_trigger:triggered\", 0.0) == 1.0:\n",
    "                    detected_events.append({\n",
    "                        'context': list(event_buffer),\n",
    "                        'trigger_timestamp': trigger_timestamp,\n",
    "                        'global_index': middle_sample.get(\"global:index\", -1)\n",
    "                    })\n",
    "        \n",
    "        # Update progress bar at 1% intervals\n",
    "        current_progress = (i + chunk_size) / total_samples\n",
    "        progress_percent = int(current_progress * 100)\n",
    "        \n",
    "        if progress_percent > last_progress_percent:\n",
    "            bar_length = 50\n",
    "            filled_length = int(bar_length * current_progress)\n",
    "            bar = '█' * filled_length + '-' * (bar_length - filled_length)\n",
    "            sys.stdout.write(f'\\rProgress: |{bar}| {progress_percent}% Complete | Events: {len(detected_events)}')\n",
    "            sys.stdout.flush()\n",
    "            last_progress_percent = progress_percent\n",
    "            \n",
    "    sys.stdout.write('\\n')  # Final newline after progress bar\n",
    "    return detected_events\n",
    "\n",
    "def analyze_results(events):\n",
    "    \"\"\"Analyzes and prints a summary of the detected events.\"\"\"\n",
    "    print(\"\\n📈 ANALYSIS RESULTS:\")\n",
    "    print(f\"Total events detected: {len(events)}\")\n",
    "    \n",
    "    if not events:\n",
    "        return\n",
    "\n",
    "    timestamps = [e['trigger_timestamp'] for e in events if e.get('trigger_timestamp')]\n",
    "    if len(timestamps) > 1:\n",
    "        intervals = np.diff(timestamps)\n",
    "        print(f\"  - Mean interval between events: {np.mean(intervals):.3f}s (Std: {np.std(intervals):.3f}s)\")\n",
    "\n",
    "    for i, event in enumerate(events, 1):\n",
    "        middle_sample = event['context'][len(event['context']) // 2]\n",
    "        print(f\"\\nEvent {i}:\")\n",
    "        print(f\"  - Global Index: {middle_sample.get('global:index', 'N/A')}\")\n",
    "        print(f\"  - Raw Sample: {middle_sample.get('global:raw_sample', 'N/A'):.3f}\")\n",
    "        print(f\"  - Slow Wave Z-Score: {middle_sample.get('detectors:slow_wave_detector:statistics:z_score', 'N/A'):.3f}\")\n",
    "        print(f\"  - Peak Amplitude: {middle_sample.get('detectors:slow_wave_detector:peak_z_score_amplitude', 'N/A'):.3f}\")\n",
    "\n",
    "def main(data):\n",
    "    \"\"\"Main execution function to process data.\"\"\"\n",
    "    print(\"🧠 DirectNeuralBiasing Python Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    config_path = \"config.yaml\"\n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"Config file not found. Creating a new one...\")\n",
    "        create_config_file(config_path)\n",
    "    else:\n",
    "        print(f\"Using existing config file: {config_path}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n🔧 Initializing signal processor from {config_path}...\")\n",
    "        signal_processor = dnb.PySignalProcessor.from_config_file(config_path)\n",
    "        print(\"✅ Signal processor initialized successfully!\")\n",
    "\n",
    "        events = process_data_with_progress(signal_processor, data, chunk_size=4096, context_size=2000)\n",
    "        print(\"✅ Data processing completed!\")\n",
    "\n",
    "        analyze_results(events)\n",
    "\n",
    "        print(f\"\\n🎉 Demo completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during processing: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Delete config.yaml only if this script created it\n",
    "        if created_config_file and os.path.exists(config_path):\n",
    "            try:\n",
    "                os.remove(config_path)\n",
    "                print(f\"\\nCleaned up: Deleted {config_path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error deleting config file {config_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # assume data is loaded in cells above\n",
    "    main(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34192930-3ece-42aa-b1cf-706dd7f79c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
