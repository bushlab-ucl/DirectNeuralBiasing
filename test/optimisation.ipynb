{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c5e66-6770-4975-b26b-bf6826623337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_FS  = 30_000.0       # data sample-rate (Hz)\n",
    "MRK_FS   =    512.0       # marker sample-rate (Hz)\n",
    "\n",
    "def _parse_mrk(path: Path) -> np.ndarray:\n",
    "    \"\"\"return 512 Hz marker indices from a .mrk file\"\"\"\n",
    "    out = []\n",
    "    with path.open() as f:\n",
    "        next(f)                          # skip BrainVision header\n",
    "        for ln in f:\n",
    "            sp = ln.strip().split()\n",
    "            if len(sp) == 3:\n",
    "                out.append(int(sp[0]))\n",
    "    return np.asarray(out, int)\n",
    "\n",
    "def mrk_count_by_patient(patient_ids, fraction=1.0):\n",
    "    \"\"\"print #events in first *fraction* of each patient’s recording\"\"\"\n",
    "    for pid in patient_ids:\n",
    "        # load EEG to get total length\n",
    "        n_samples = np.load(DATA_DIR / f\"Patient{pid}EEG.npy\")[0].size\n",
    "        trunc_len = int(n_samples * fraction)\n",
    "\n",
    "        # load marker indices (512 Hz) and convert to 30 kHz space\n",
    "        mrk_path  = DATA_DIR / f\"Patient{pid:02d}_OfflineMrk.mrk\"\n",
    "        mrk_30k   = (_parse_mrk(mrk_path) * DATA_FS / MRK_FS).astype(int)\n",
    "\n",
    "        n_events  = np.sum(mrk_30k < trunc_len)\n",
    "        print(f\"Patient {pid}: {n_events} events in first {fraction:.0%} of data\")\n",
    "\n",
    "# example: first 10 % on patients 2,3,4,6,7\n",
    "mrk_count_by_patient([2, 3, 4, 6, 7], fraction=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b85d4-ab4b-48f8-9de7-59085196232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_FS = 30_000.0  # Data sample-rate (Hz)\n",
    "MRK_FS = 512.0      # Marker sample-rate (Hz)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def _parse_mrk(path: Path) -> np.ndarray:\n",
    "    \"\"\"Return 512 Hz marker indices from a .mrk file\"\"\"\n",
    "    out = []\n",
    "    if not path.exists():\n",
    "        return np.array([], dtype=int)\n",
    "    with path.open() as f:\n",
    "        next(f)  # Skip BrainVision header\n",
    "        # CORRECTED: Iterate over the file object line by line\n",
    "        for ln in f:\n",
    "            # Based on your first script, the marker index is the first element\n",
    "            sp = ln.strip().split()\n",
    "            if sp:  # Ensure the line is not empty after stripping\n",
    "                try:\n",
    "                    out.append(int(sp[0]))\n",
    "                except (ValueError, IndexError):\n",
    "                    # This will skip lines that don't start with a valid integer\n",
    "                    continue\n",
    "    return np.asarray(out, dtype=int)\n",
    "\n",
    "def format_seconds(sec: float) -> str:\n",
    "    \"\"\"Formats seconds into a human-readable HH:MM:SS.ms string.\"\"\"\n",
    "    if sec < 0:\n",
    "        return \"00:00:00.000\"\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{int(h):02d}:{int(m):02d}:{s:06.3f}\"\n",
    "\n",
    "# --- Main Verification Logic ---\n",
    "def verify_marker_data_alignment(patient_ids: list):\n",
    "    \"\"\"\n",
    "    Checks if the last event in a patient's .mrk file aligns with the\n",
    "    total length of their .npy data file.\n",
    "    \"\"\"\n",
    "    print(f\"{'Patient':<10} | {'Status':<10} | {'Last Marker Time':<18} | {'Data Duration':<18} | {'Details'}\")\n",
    "    print(f\"{'-'*10} | {'-'*10} | {'-'*18} | {'-'*18} | {'-'*40}\")\n",
    "\n",
    "    for pid in patient_ids:\n",
    "        try:\n",
    "            # 1. Load EEG data to get its total length in samples\n",
    "            eeg_path = DATA_DIR / f\"Patient{pid}EEG.npy\"\n",
    "            if not eeg_path.exists():\n",
    "                print(f\"{pid:<10} | {'❌ ERROR':<10} | {'N/A':<18} | {'N/A':<18} | EEG data file not found\")\n",
    "                continue\n",
    "\n",
    "            n_samples = np.load(eeg_path)[0].size\n",
    "            data_duration_sec = n_samples / DATA_FS\n",
    "\n",
    "            # 2. Load and process marker indices\n",
    "            mrk_path = DATA_DIR / f\"Patient{pid:02d}_OfflineMrk.mrk\"\n",
    "            mrk_512 = _parse_mrk(mrk_path)\n",
    "\n",
    "            if not mrk_512.size:\n",
    "                print(f\"{pid:<10} | {'⚠️ WARNING':<10} | {'N/A':<18} | {format_seconds(data_duration_sec):<18} | No markers found in file\")\n",
    "                continue\n",
    "\n",
    "            # 3. Convert marker indices from 512 Hz to 30 kHz sample rate\n",
    "            mrk_30k = (mrk_512 * DATA_FS / MRK_FS).astype(int)\n",
    "\n",
    "            # 4. Perform the checks\n",
    "            last_marker_sample = mrk_30k.max()\n",
    "            last_marker_sec = last_marker_sample / DATA_FS\n",
    "\n",
    "            # Check for any markers that fall outside the data file's duration\n",
    "            n_outside_bounds = np.sum(mrk_30k >= n_samples)\n",
    "\n",
    "            status = \"✅ OK\"\n",
    "            details = f\"{len(mrk_30k)} markers found.\"\n",
    "\n",
    "            if n_outside_bounds > 0:\n",
    "                status = \"❌ ERROR\"\n",
    "                details = f\"{n_outside_bounds} markers found AFTER data ends.\"\n",
    "            # Warn if the last marker occurs very early in the recording\n",
    "            elif last_marker_sec < (data_duration_sec * 0.8):\n",
    "                status = \"⚠️ WARNING\"\n",
    "                details = \"Last marker occurs unusually early.\"\n",
    "\n",
    "            print(f\"{pid:<10} | {status:<10} | {format_seconds(last_marker_sec):<18} | {format_seconds(data_duration_sec):<18} | {details}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{pid:<10} | {'❌ ERROR':<10} | {'-':<18} | {'-':<18} | Exception: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for full error details\n",
    "\n",
    "# --- Run Verification ---\n",
    "if __name__ == \"__main__\":\n",
    "    patient_list = [2, 3, 4, 6, 7] # The patients from your example\n",
    "    verify_marker_data_alignment(patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182924b2-4ea4-4207-b07b-b9813508ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "DirectNeuralBiasing – multi-threaded grid-search with progress bars + event plots\n",
    "==================================================================================\n",
    "\n",
    "•  Outer progress bar: hyper-parameter *combo*.\n",
    "•  Inner progress bars: per-patient chunks (processed in parallel), shows live TP / FP counts.\n",
    "•  Fraction switch lets you smoke-test on 1 % of every EEG file.\n",
    "\n",
    "Adjust **PARAM_GRID**, **PATIENT_IDS**, **FRACTION**, or **MAX_WORKERS** at the top.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import itertools, os, csv, json, tempfile, threading\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np, yaml, matplotlib.pyplot as plt\n",
    "import direct_neural_biasing as dnb\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.signal as sig \n",
    "\n",
    "# ─────────────────────—— user knobs ─────────────────────────────────────────\n",
    "PATIENT_IDS      = [2,3,4,6,7]               # full: [2,3,4,6,7]\n",
    "DATA_DIR         = Path(\"data\")\n",
    "FRACTION         = 1.0                 #\n",
    "MAX_WORKERS      = 4                   # number of parallel threads\n",
    "\n",
    "PARAM_GRID = {\n",
    "    \"z_score_threshold\"      : [2.5, 3.0, 3.5, 4.0],        # keep full range\n",
    "    \"sinusoidness_threshold\" : [0.4, 0.5, 0.6, 0.7],        # mid band only\n",
    "    \"check_sinusoidness\"     : [True, False],          # still a switch\n",
    "\n",
    "    # freeze slow-wave band for now\n",
    "    \"f_low\"   : [0.25],\n",
    "    \"f_high\"  : [4.0],\n",
    "\n",
    "    # explore duration only when check_sin is True\n",
    "    \"min_wave_ms\" : [250.0],                           # fixed\n",
    "    \"max_wave_ms\" : [1000.0],\n",
    "\n",
    "    # simpler IED + refractory\n",
    "    \"z_ied_threshold\" : [1.5, 2.0, 2.5],\n",
    "    \"refrac_ms\"       : [2500.0],\n",
    "}\n",
    "\n",
    "\n",
    "DATA_FS      = 30_000.0;  MRK_FS = 512.0\n",
    "TOLERANCE_MS =       50\n",
    "CTX_MS       =       800          # ± ms context for plots\n",
    "\n",
    "SHOW_PLOTS   = True\n",
    "MAX_PLOTS    = 4                  # per patient\n",
    "PRINT_EVENTS = True\n",
    "MAX_EVENT_PRINT = 6\n",
    "\n",
    "RESULTS_DIR = Path(\"results\"); RESULTS_DIR.mkdir(exist_ok=True)\n",
    "OUT_CSV     = RESULTS_DIR / \"grid_metrics_mt.csv\"\n",
    "\n",
    "# Thread-safe CSV writer\n",
    "csv_lock = threading.Lock()\n",
    "plot_queue = Queue()  # Queue for deferred plotting (matplotlib isn't thread-safe)\n",
    "\n",
    "# ─────────────────────—— data helpers ──────────────────────────────────────\n",
    "def _parse_mrk(p: Path):\n",
    "    with p.open() as f:\n",
    "        next(f)\n",
    "        return np.asarray([int(l.split()[0]) for l in f if l.split()], int)\n",
    "\n",
    "def _mrk512_to_30k(idx): return (idx * DATA_FS / MRK_FS).astype(int)\n",
    "\n",
    "def load_patient(pid:int, frac:float)->Tuple[np.ndarray,Dict[int,int]]:\n",
    "    sig = np.load(DATA_DIR/f\"Patient{pid}EEG.npy\")[0]\n",
    "    sig = sig[: int(len(sig)*frac)]\n",
    "    mrk = _parse_mrk(DATA_DIR/f\"Patient{pid:02d}_OfflineMrk.mrk\")\n",
    "    gt  = dict(zip(_mrk512_to_30k(mrk), mrk))\n",
    "    gt  = {k:v for k,v in gt.items() if k < len(sig)}   # drop markers > slice\n",
    "    return sig, gt\n",
    "\n",
    "# ─────────────────────—— plotting ──────────────────────────────────────────\n",
    "def _plot(sig_raw: np.ndarray,\n",
    "          center: int,\n",
    "          gt_idx: np.ndarray,\n",
    "          title: str,\n",
    "          f_low: float = 0.25,         # pass in grid values if varied\n",
    "          f_high: float = 4.0):\n",
    "    \"\"\"\n",
    "    Show raw (grey) and band-pass-filtered (blue) signal ±CTX_MS around `center`.\n",
    "    Skips plot if window has < 10 samples or is all-zero.\n",
    "    \"\"\"\n",
    "    if center is None or center >= len(sig_raw):\n",
    "        return\n",
    "    ctx = int(CTX_MS / 1000 * DATA_FS)\n",
    "    L, R = max(0, center - ctx), min(len(sig_raw), center + ctx)\n",
    "    if R - L < 10 or np.allclose(sig_raw[L:R], 0, atol=1e-12):\n",
    "        return\n",
    "\n",
    "    # ── design Butterworth band-pass (2nd order each side) ──────────────────\n",
    "    nyq = 0.5 * DATA_FS\n",
    "    b, a = sig.butter(\n",
    "        N=2,\n",
    "        Wn=[f_low / nyq, f_high / nyq],\n",
    "        btype=\"bandpass\",\n",
    "        analog=False,\n",
    "    )\n",
    "    filt = sig.filtfilt(b, a, sig_raw[L:R])\n",
    "\n",
    "    t = (np.arange(L, R) - center) / DATA_FS * 1000  # ms axis\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(t, sig_raw[L:R], color=\"lightgray\", lw=0.6, label=\"raw\")\n",
    "    plt.plot(t, filt, color=\"C0\", lw=1.0, label=\"band-passed\")\n",
    "    plt.axvline(0, c=\"r\", lw=1)\n",
    "\n",
    "    # GT markers inside window\n",
    "    in_win = (gt_idx >= L) & (gt_idx < R)\n",
    "    plt.vlines((gt_idx[in_win] - center) / DATA_FS * 1000,\n",
    "               *plt.ylim(), colors=\"g\", linestyles=\":\", label=\"GT\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"time (ms)\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ─────────────────────—— metric / live inner bar ───────────────────────────\n",
    "def eval_patient(proc, sig, gt_map, desc:str, position:int=0):\n",
    "    \"\"\"\n",
    "    Evaluate a single patient with a given processor configuration.\n",
    "    position: tqdm position for nested progress bars in multi-threaded context\n",
    "    \"\"\"\n",
    "    tol = int(TOLERANCE_MS/1000*DATA_FS)\n",
    "    gt_idx = np.fromiter(gt_map.keys(), int)\n",
    "    matched = np.zeros(gt_idx.size,bool)\n",
    "    tp=fp=0; events=[]\n",
    "    bar = tqdm(range(0,len(sig),4096), leave=False, desc=desc, unit=\"chunk\", position=position)\n",
    "\n",
    "    for off in bar:\n",
    "        out,_ = proc.run_chunk(sig[off:off+4096].tolist())\n",
    "        for o in out:\n",
    "            if o.get(\"detectors:slow_wave_detector:detected\")!=1: continue\n",
    "            det = int(o.get(\"detectors:slow_wave_detector:wave_start_index\",-1))\n",
    "            i = int(np.abs(gt_idx-det).argmin()) if gt_idx.size else None\n",
    "            if i is not None and abs(gt_idx[i]-det)<=tol and not matched[i]:\n",
    "                matched[i]=True; tp+=1; events.append((\"TP\",det,gt_idx[i]))\n",
    "            else:\n",
    "                fp+=1; events.append((\"FP\",det,None))\n",
    "        bar.set_postfix(tp=tp, fp=fp)\n",
    "    fn_idx = gt_idx[~matched]\n",
    "    events.extend([(\"FN\",None,x) for x in fn_idx])\n",
    "    bar.close()\n",
    "    return tp,fp,len(fn_idx),events\n",
    "\n",
    "# ─────────────────────—— YAML helper ───────────────────────────────────────\n",
    "BASE_CFG = {\n",
    "    \"processor\":{\"verbose\":False,\"fs\":DATA_FS,\"channel\":1,\n",
    "                 \"enable_debug_logging\":False},\n",
    "    \"filters\":{\"bandpass_filters\":[\n",
    "        {\"id\":\"slow_wave_filter\",\"f_low\":0.25,\"f_high\":4.0},\n",
    "        {\"id\":\"ied_filter\",\"f_low\":80.0,\"f_high\":120.0}]},\n",
    "    \"detectors\":{\"wave_peak_detectors\":[\n",
    "        {\"id\":\"slow_wave_detector\",\"filter_id\":\"slow_wave_filter\",\n",
    "         \"z_score_threshold\":2.5,\"sinusoidness_threshold\":0.6,\n",
    "         \"check_sinusoidness\":True,\"wave_polarity\":\"downwave\",\n",
    "         \"min_wave_length_ms\":250.0,\"max_wave_length_ms\":1000.0},\n",
    "        {\"id\":\"ied_detector\",\"filter_id\":\"ied_filter\",\"z_score_threshold\":2.5,\n",
    "         \"sinusoidness_threshold\":0.0,\"check_sinusoidness\":False,\n",
    "         \"wave_polarity\":\"upwave\"}]},\n",
    "    \"triggers\":{\"pulse_triggers\":[{\n",
    "        \"id\":\"pulse_trigger\",\"activation_detector_id\":\"slow_wave_detector\",\n",
    "        \"inhibition_detector_id\":\"ied_detector\",\n",
    "        \"inhibition_cooldown_ms\":2500.0,\"pulse_cooldown_ms\":0}]}\n",
    "}\n",
    "\n",
    "def make_cfg(over:dict):\n",
    "    cfg=deepcopy(BASE_CFG)\n",
    "    det=cfg[\"detectors\"][\"wave_peak_detectors\"][0]\n",
    "    for k,v in over.items():\n",
    "        if k==\"z_score_threshold\":det[\"z_score_threshold\"]=v\n",
    "        elif k==\"sinusoidness_threshold\":det[\"sinusoidness_threshold\"]=v\n",
    "        elif k==\"check_sinusoidness\":det[\"check_sinusoidness\"]=v\n",
    "        elif k==\"f_low\":cfg[\"filters\"][\"bandpass_filters\"][0][\"f_low\"]=v\n",
    "        elif k==\"f_high\":cfg[\"filters\"][\"bandpass_filters\"][0][\"f_high\"]=v\n",
    "        elif k==\"min_wave_ms\":det[\"min_wave_length_ms\"]=v\n",
    "        elif k==\"max_wave_ms\":det[\"max_wave_length_ms\"]=v\n",
    "        elif k==\"z_ied_threshold\":cfg[\"detectors\"][\"wave_peak_detectors\"][1][\"z_score_threshold\"]=v\n",
    "        elif k==\"refrac_ms\":\n",
    "            cfg[\"triggers\"][\"pulse_triggers\"][0][\"inhibition_cooldown_ms\"]=v\n",
    "    return cfg\n",
    "\n",
    "# ─────────────────────—— worker function ───────────────────────────────────\n",
    "def process_patient(args):\n",
    "    \"\"\"Worker function to process a single patient\"\"\"\n",
    "    cid, params, pid, cfg_path, position = args\n",
    "    \n",
    "    # Create processor from config\n",
    "    proc = dnb.PySignalProcessor.from_config_file(cfg_path)\n",
    "    \n",
    "    # Load patient data\n",
    "    sig, gt = load_patient(pid, FRACTION)\n",
    "    desc = f\"P{pid} combo{cid}\"\n",
    "    \n",
    "    # Evaluate\n",
    "    tp, fp, fn, events = eval_patient(proc, sig, gt, desc, position)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    prec = tp/(tp+fp) if tp+fp else 0\n",
    "    rec = tp/(tp+fn) if tp+fn else 0\n",
    "    \n",
    "    # Prepare plot data if needed\n",
    "    plot_data = None\n",
    "    if SHOW_PLOTS:\n",
    "        plot_data = {\n",
    "            'sig': sig,\n",
    "            'gt': gt,\n",
    "            'events': events[:MAX_PLOTS + 1],  # Include one extra for FN\n",
    "            'pid': pid,\n",
    "            'cid': cid,\n",
    "            'params': params\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'cid': cid,\n",
    "        'params': params,\n",
    "        'pid': pid,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'prec': prec,\n",
    "        'rec': rec,\n",
    "        'events': events[:MAX_EVENT_PRINT] if PRINT_EVENTS else [],\n",
    "        'plot_data': plot_data\n",
    "    }\n",
    "\n",
    "# ─────────────────────—— grid driver ───────────────────────────────────────\n",
    "def run_grid():\n",
    "    fresh = not OUT_CSV.exists()\n",
    "    \n",
    "    # Open CSV file and write header if needed\n",
    "    with OUT_CSV.open(\"a\", newline=\"\") as fh:\n",
    "        wr = csv.writer(fh)\n",
    "        if fresh: \n",
    "            wr.writerow([\"combo\",\"params\",\"patient\",\"tp\",\"fp\",\"fn\",\n",
    "                         \"precision\",\"recall\"])\n",
    "    \n",
    "    keys = list(PARAM_GRID.keys())\n",
    "    combos = list(itertools.product(*PARAM_GRID.values()))\n",
    "    outer = tqdm(combos, desc=\"param-sets\")\n",
    "    \n",
    "    for cid, vals in enumerate(outer, 1):\n",
    "        params = dict(zip(keys, vals))\n",
    "        outer.set_postfix(params=params)\n",
    "        cfg = make_cfg(params)\n",
    "        \n",
    "        # Create temporary config file\n",
    "        with tempfile.NamedTemporaryFile(\"w\", suffix=\".yaml\", delete=False) as tmp:\n",
    "            yaml.dump(cfg, tmp)\n",
    "            cfg_path = tmp.name\n",
    "        \n",
    "        try:\n",
    "            # Prepare tasks for all patients\n",
    "            tasks = []\n",
    "            for i, pid in enumerate(PATIENT_IDS):\n",
    "                tasks.append((cid, params, pid, cfg_path, i+1))\n",
    "            \n",
    "            # Process patients in parallel\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                # Submit all tasks\n",
    "                future_to_task = {executor.submit(process_patient, task): task \n",
    "                                  for task in tasks}\n",
    "                \n",
    "                # Collect results as they complete\n",
    "                results = []\n",
    "                for future in as_completed(future_to_task):\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        results.append(result)\n",
    "                    except Exception as e:\n",
    "                        task = future_to_task[future]\n",
    "                        print(f\"Error processing patient {task[2]}: {e}\")\n",
    "                \n",
    "                # Sort results by patient ID to maintain consistent order\n",
    "                results.sort(key=lambda x: x['pid'])\n",
    "                \n",
    "                # Process results\n",
    "                for result in results:\n",
    "                    # Write to CSV (thread-safe)\n",
    "                    with csv_lock:\n",
    "                        with OUT_CSV.open(\"a\", newline=\"\") as fh:\n",
    "                            wr = csv.writer(fh)\n",
    "                            wr.writerow([\n",
    "                                result['cid'],\n",
    "                                json.dumps(result['params']),\n",
    "                                result['pid'],\n",
    "                                result['tp'],\n",
    "                                result['fp'],\n",
    "                                result['fn'],\n",
    "                                result['prec'],\n",
    "                                result['rec']\n",
    "                            ])\n",
    "                    \n",
    "                    # Print summary\n",
    "                    print(f\"\\n▶ combo {cid}/{len(combos)} params={params}\"\n",
    "                          f\"\\n   patient {result['pid']}: TP={result['tp']} FP={result['fp']} FN={result['fn']} \"\n",
    "                          f\"prec={result['prec']:.3f} rec={result['rec']:.3f}\")\n",
    "                    \n",
    "                    # Print events\n",
    "                    if PRINT_EVENTS:\n",
    "                        for lab, det, gt_i in result['events']:\n",
    "                            if lab == \"TP\": \n",
    "                                print(f\"     TP det={det} gt={gt_i}\")\n",
    "                            elif lab == \"FP\":\n",
    "                                print(f\"     FP det={det}\")\n",
    "                            else:\n",
    "                                print(f\"     FN miss gt={gt_i}\")\n",
    "                    \n",
    "                    # Queue plots for later (matplotlib isn't thread-safe)\n",
    "                    if result['plot_data']:\n",
    "                        plot_queue.put(result['plot_data'])\n",
    "                \n",
    "                # Process all queued plots after parallel processing\n",
    "                while not plot_queue.empty():\n",
    "                    plot_data = plot_queue.get()\n",
    "                    sig = plot_data['sig']\n",
    "                    gt = plot_data['gt']\n",
    "                    events = plot_data['events']\n",
    "                    pid = plot_data['pid']\n",
    "                    cid = plot_data['cid']\n",
    "                    params = plot_data['params']\n",
    "                    \n",
    "                    plotted = 0\n",
    "                    for lab, det, gt_i in events:\n",
    "                        if plotted >= MAX_PLOTS: \n",
    "                            break\n",
    "                        if lab in (\"TP\", \"FP\"):\n",
    "                            _plot(sig, det, np.fromiter(gt.keys(), int),\n",
    "                                  f\"{lab}   P{pid}   cfg{cid}\",\n",
    "                                  params.get('f_low', 0.25),\n",
    "                                  params.get('f_high', 4.0))\n",
    "                            plotted += 1\n",
    "                    \n",
    "                    # Plot one FN if we have room\n",
    "                    if plotted < MAX_PLOTS:\n",
    "                        for lab, det, gt_i in events:\n",
    "                            if lab == \"FN\":\n",
    "                                _plot(sig, gt_i, np.fromiter(gt.keys(), int),\n",
    "                                      f\"FN   P{pid}   cfg{cid}\",\n",
    "                                      params.get('f_low', 0.25),\n",
    "                                      params.get('f_high', 4.0))\n",
    "                                break\n",
    "        \n",
    "        finally:\n",
    "            # Clean up temp file\n",
    "            os.remove(cfg_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting multi-threaded grid search with {MAX_WORKERS} workers...\")\n",
    "    run_grid()\n",
    "    print(f\"\\n✅ finished – rows saved to {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ff2fa-3e05-47e8-ac6c-206cd13e2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Summarise grid-search results and pick the most selective parameter set\n",
    "======================================================================\n",
    "\n",
    "*   Reads **results/grid_metrics.csv** (rows = patient × param-combo).\n",
    "*   Aggregates TP/FP/FN across all patients.\n",
    "*   Computes precision (= selectivity), recall, and F1.\n",
    "*   Ranks by **precision first** (tie-break by recall, then F1).\n",
    "*   Prints the top 20 and writes a full summary CSV.\n",
    "*   Dumps the best parameters as JSON for easy reuse.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "\n",
    "    python summarise_grid.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────────\n",
    "CSV_PATH   = Path(\"results/grid_metrics.csv\")\n",
    "SUMMARY_CSV = CSV_PATH.with_suffix(\".combo_summary.csv\")\n",
    "BEST_JSON   = CSV_PATH.with_suffix(\".best_params.json\")\n",
    "\n",
    "# ─── load ───────────────────────────────────────────────────────────────────\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"{CSV_PATH} not found – run the grid search first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# turn params JSON into a canonical string for grouping\n",
    "df[\"params_str\"] = df[\"params\"].apply(\n",
    "    lambda s: json.dumps(json.loads(s), sort_keys=True)\n",
    ")\n",
    "\n",
    "# ─── aggregate ──────────────────────────────────────────────────────────────\n",
    "agg = (\n",
    "    df.groupby(\"params_str\")\n",
    "      .agg(tp=(\"tp\", \"sum\"),\n",
    "           fp=(\"fp\", \"sum\"),\n",
    "           fn=(\"fn\", \"sum\"))\n",
    ")\n",
    "\n",
    "agg[\"precision\"] = agg[\"tp\"] / (agg[\"tp\"] + agg[\"fp\"])\n",
    "agg[\"recall\"]    = agg[\"tp\"] / (agg[\"tp\"] + agg[\"fn\"])\n",
    "agg[\"f1\"]        = 2 * agg[\"precision\"] * agg[\"recall\"] / \\\n",
    "                   (agg[\"precision\"] + agg[\"recall\"])\n",
    "\n",
    "# ─── rank: maximise selectivity (precision) first ───────────────────────────\n",
    "ranked = (\n",
    "    agg.sort_values(\n",
    "        by=[\"precision\", \"recall\", \"f1\"],\n",
    "        ascending=[False,   False,    False]\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─── display top 20 ─────────────────────────────────────────────────────────\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "print(\"\\n🏆  TOP 20 PARAMETER SETS (by precision)\\n\")\n",
    "print(\n",
    "    ranked[[\"tp\", \"fp\", \"fn\", \"precision\", \"recall\", \"f1\"]]\n",
    "    .head(20)\n",
    "    .to_string(float_format=\"%.3f\")\n",
    ")\n",
    "\n",
    "# ─── write full summary ─────────────────────────────────────────────────────\n",
    "ranked.to_csv(SUMMARY_CSV)\n",
    "print(f\"\\n📄  Full combo summary saved to {SUMMARY_CSV}\")\n",
    "\n",
    "# ─── extract & save the best parameter set ─────────────────────────────────-\n",
    "best_str = ranked.index[0]\n",
    "best_params = json.loads(best_str)\n",
    "with BEST_JSON.open(\"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(f\"⭐  Best (most selective) params written to {BEST_JSON}\\n\")\n",
    "print(\"Best params:\\n\", json.dumps(best_params, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ad3ef-35bb-45a7-ba31-2649e65b94c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
